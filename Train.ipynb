{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538bed51-4c6e-43a5-b7a5-ce123e086de9",
   "metadata": {},
   "source": [
    "# Full Train BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106a901-05b2-419d-9844-25c221f64570",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07ccf7f1-2955-4ff9-b60e-015cf1833cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and tokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070032e-45b3-4305-9372-304fd9e76db0",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d886d757-3d2d-48ac-b88d-3bc759ea33d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"premise\"], example[\"hypothesis\"], truncation=True)\n",
    "# batch processing\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# tokenized_datasets[\"train\"][\"label\"][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d57700b9-d7c1-4e3b-beea-c88a79db238d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# post-processing to prepare for dataloader\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"premise\", \"hypothesis\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\") # Pytorch tensors\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6e02a3-55af-4b12-a542-5f1813975908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No padding done yet\n",
    "tokenized_datasets[\"train\"][\"attention_mask\"][:1]\n",
    "tokenized_datasets[\"train\"][\"labels\"][:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa0aa1-a893-4db6-a550-5056efc3d214",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e5555f-ffc3-4b8b-98ac-dd8111c36354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "# For dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Prepare the data in batch size of 8 with dynamic padding, shuffles at each epoch\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# combine both validation sets\n",
    "combined_validation = concatenate_datasets([\n",
    "    tokenized_datasets[\"validation_matched\"],\n",
    "    tokenized_datasets[\"validation_mismatched\"]\n",
    "])\n",
    "\n",
    "# Prepare eval data in batch size of 8\n",
    "eval_dataloader = DataLoader(\n",
    "    combined_validation, batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "136c729c-9c16-4249-890c-96cbdc9720ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(combined_validation[\"labels\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "430283f1-3681-42a1-86fe-4b7d40ac07f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 62]),\n",
       " 'token_type_ids': torch.Size([8, 62]),\n",
       " 'attention_mask': torch.Size([8, 62])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the first batch\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f0f98c-9a3c-4fb5-8fd1-56e225ab7ed5",
   "metadata": {},
   "source": [
    "## Load BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98f966ef-e29a-4416-9211-7013e8621265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the base model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "# Weights are randomnized\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad522fa-619f-47c5-8aa6-328de4153de1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "sample = tokenized_datasets[\"train\"][:8]\n",
    "batch = data_collator(sample)\n",
    "# print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b78704f-c49b-4b87-a4e2-cc6db55c2b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2584, grad_fn=<NllLossBackward0>) torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985ffd0-72e0-4cb5-9761-0878e70e33df",
   "metadata": {},
   "source": [
    "## Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80fbd281-ab28-4d95-a957-2ceea66524e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimiser\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5) # State learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4543330-0393-4bec-b7eb-0201cc45e229",
   "metadata": {},
   "source": [
    "## Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a5cc7d-f606-4689-80c9-f6b9dd5c07f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98176\n"
     ]
    }
   ],
   "source": [
    "# Define scheduler to change learning rate\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 2\n",
    "# num epochs * num of batches\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", # Type of scheduler (linear decay)\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0, # No warm-up period, meaning the learning rate starts at the maximum value right away and decreases linearly\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843d52a-01f4-4ee5-9418-bd48d6c7ca57",
   "metadata": {},
   "source": [
    "## Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cc528ae-aa47-49e4-8506-66db17d08203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to GPU if avail or CPU\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce44875-a857-4501-a3b7-f30a577c2168",
   "metadata": {},
   "source": [
    "## Set Model to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69acea8a-7d78-4137-9665-c3e47071617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the tqdm library for progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()  # This ensures layers like dropout are active during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8cb973-6c5b-404c-9b2c-42f618c467f4",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c6614-d5cf-403e-9749-62055dfc1592",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop over each epoch (the entire training dataset will be processed for each epoch)\n",
    "for epoch in range(num_epochs):  # num_epochs is the number of times you want to loop through the dataset\n",
    "    # Loop over each batch in the training dataset\n",
    "    for step, batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", total=len(train_dataloader)):  # train_dataloader yields batches of training data\n",
    "        # Move each element in the batch to the device (GPU/CPU)\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}  # Move tensors to device\n",
    "\n",
    "        # Forward pass: pass the batch through the model\n",
    "        outputs = model(**batch)  # Forward pass to get predictions, loss, etc.\n",
    "\n",
    "        # Get the loss value from the model's outputs (assuming the model returns a loss)\n",
    "        loss = outputs.loss  # Extract the loss from the model's outputs\n",
    "\n",
    "        # Backward pass: Compute the gradients for backpropagation\n",
    "        loss.backward()  # Backpropagate the loss to compute gradients\n",
    "\n",
    "        # Step the optimizer to update model parameters using the gradients\n",
    "        optimizer.step()  # Update the model's weights using the computed gradients\n",
    "\n",
    "        # Update the learning rate according to the learning rate scheduler\n",
    "        lr_scheduler.step()  # Adjust the learning rate based on the scheduler\n",
    "\n",
    "        # Zero out gradients to prevent them from accumulating in the next iteration\n",
    "        optimizer.zero_grad()  # Clear the gradients after the update\n",
    "\n",
    "        # Update the progress bar by one step (one batch processed)\n",
    "        progress_bar.update(1)  # Increment the progress bar by 1 (one batch done)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Step {step}/{len(train_dataloader)}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708dc435-56f1-4b81-8bdf-95e984993f4f",
   "metadata": {},
   "source": [
    "## Validation / Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "256b22e0-d88c-47e8-8e1c-27f3409c798a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8310683564920853}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the evaluate library for metrics\n",
    "import evaluate\n",
    "\n",
    "# Load the 'mnli' metric from the GLUE dataset\n",
    "metric = evaluate.load(\"glue\", \"mnli\")  \n",
    "\n",
    "# Set the model to evaluation mode (deactivates dropout and other training behaviors)\n",
    "model.eval()  # This ensures layers like dropout are deactivated, making predictions deterministic\n",
    "\n",
    "# Loop over each batch in the evaluation dataset (eval_dataloader)\n",
    "for batch in eval_dataloader:  # eval_dataloader yields batches of evaluation data\n",
    "    # Move each element in the batch to the device (GPU/CPU)\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}  # Move tensors to device\n",
    "\n",
    "    # Disable gradient calculation as we are in evaluation mode\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        # Forward pass: pass the batch through the model\n",
    "        outputs = model(**batch)  # Forward pass to get model outputs (logits)\n",
    "\n",
    "    # Get the logits (raw predictions before applying any activation function like softmax)\n",
    "    logits = outputs.logits  # Extract the logits from the model's outputs\n",
    "\n",
    "    # Convert logits to predictions by selecting the index with the highest value\n",
    "    predictions = torch.argmax(logits, dim=-1)  # Get the predicted class (index of max logit)\n",
    "\n",
    "    # Add the predictions and the true labels to the metric for evaluation\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])  # Add batch predictions and true labels\n",
    "\n",
    "# After accumulating all batches, compute the final metric result (e.g., accuracy, F1 score, etc.)\n",
    "metric.compute()  # Compute and return the final evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789be00-388f-4391-bd31-af47f842c4ef",
   "metadata": {},
   "source": [
    "## Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4af33f-d0c9-4133-ae9a-818cd7d9b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "output_dir = \"C:/Users/steve/HuggingFace Models/BERT_MNLI_model\" \n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
